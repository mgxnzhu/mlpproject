{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(20180201)\n",
    "DEFAULT_SEED = 123456\n",
    "DATA_DIR = \"../data/\"\n",
    "DATA_FILE = \"creditcard.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR+DATA_FILE)\n",
    "stats = pd.DataFrame({'mean':data.mean(), 'std':data.std(), 'skew':data.skew()})\n",
    "\n",
    "# normalize the data\n",
    "data['Amount'] = data['Amount'].apply(lambda x: np.log(x+1e-6))\n",
    "data.loc[:,'V1':'Amount'] = data.loc[:,'V1':'Amount'].apply(lambda x: (x-x.mean())/x.std())\n",
    "#data.loc[:,'V1':'V28'] = data.loc[:,'V1':'V28'].apply(lambda x: (x-x.min())/(x.std(x.max()-x.min())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histograms of data in each axises\n",
    "plot=False\n",
    "if plot:\n",
    "    pos_data = data[data[\"Class\"]==1]\n",
    "    neg_data = data[data[\"Class\"]==0]\n",
    "    for key in data.keys()[1:-1]: # for all principle components\n",
    "        fig = plt.figure(figsize=(4,3))\n",
    "        plt.hist(pos_data[key], histtype=\"step\", normed=True, bins=50, range=(-1,1), label=\"pos\")\n",
    "        plt.hist(neg_data[key], histtype=\"step\", normed=True, bins=50, range=(-1,1), label=\"neg\")\n",
    "        plt.xlim(-1,1)\n",
    "        plt.title(key)\n",
    "        plt.legend()\n",
    "        fig.savefig(key+\"_Scaled.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_DataFrame(data, label, frac, rng=None, oversample=None):\n",
    "    '''\n",
    "    To split the data into two part, and remain the same event propotion in the results.\n",
    "    Use simple oversampling to preprocess the imblanced dataset.\n",
    "    \n",
    "    Input:\n",
    "        data (pd.DataFrame): the Input data\n",
    "        label (string): the event class label\n",
    "        frac (float): the propotion of the first part\n",
    "        rng (np.RandomState): NumPy random state\n",
    "        oversample (float): a constant to control how many times to duplicate (usually < 1)\n",
    "    \n",
    "    Return:\n",
    "        subdf1: the first part of the data\n",
    "        subdf2: the second part of the data\n",
    "    '''\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(DEFAULT_SEED)\n",
    "    subdf1 = pd.DataFrame()\n",
    "    subdf2 = pd.DataFrame()\n",
    "    count = data[label].value_counts()\n",
    "    if oversample is None:\n",
    "        oversample = np.ones_like(count)\n",
    "    else:\n",
    "        oversample = np.int32(np.ceil(count.max()/count * oversample))\n",
    "    for val, amplify in zip(count.keys(), oversample):\n",
    "        df = data[data[label]==val]\n",
    "        mask = rng.rand(len(df)) < frac\n",
    "        subdf1 = pd.concat([df[mask]] * amplify + [subdf1]).sample(frac=1, random_state=rng).reset_index(drop=True)\n",
    "        subdf2 = pd.concat([df[~mask]] * amplify + [subdf2]).sample(frac=1, random_state=rng).reset_index(drop=True)\n",
    "    return (subdf1, subdf2)\n",
    "\n",
    "# train : test : valid = 0.6 : 0.2 : 0.2 \n",
    "train_df, dump = split_DataFrame(data, label='Class', frac=0.6, rng=rng, oversample=0.7)\n",
    "test_df, valid_df = split_DataFrame(dump, label='Class', frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.as_matrix()\n",
    "test = test_df.as_matrix()\n",
    "valid = valid_df.as_matrix()\n",
    "\n",
    "np.savez_compressed(DATA_DIR+'ccdataset.npz', train=train, test=test, valid=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
