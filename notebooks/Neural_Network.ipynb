{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "n_input = 29\n",
    "n_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 100\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(DATA_DIR+\"ccdataset.npz\") as data:\n",
    "    trainset = data['train']\n",
    "    features = trainset[:,1:-1]\n",
    "    labels_int = np.int16(trainset[:,-1])\n",
    "    labels = np.zeros((labels_int.shape[0], n_classes))\n",
    "    labels[range(labels_int.shape[0]), labels_int] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})\n",
    "\n",
    "X, Y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "def Baseline_model(x, n_classes, reuse, is_training):\n",
    "    with tf.variable_scope('Baseline', reuse=reuse):\n",
    "        layer1 = tf.layers.dense(inputs=x, units=50, activation=tf.nn.leaky_relu)\n",
    "        layer2 = tf.layers.dense(inputs=layer1, units=50, activation=tf.nn.leaky_relu)\n",
    "        layer3 = tf.layers.dense(inputs=layer2, units=50, activation=tf.nn.leaky_relu)\n",
    "        out = tf.layers.dense(inputs=layer3, units=n_classes)\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "    return out\n",
    "\n",
    "logits_train = Baseline_model(X, n_classes, reuse=False, is_training=True)\n",
    "logits_test = Baseline_model(X, n_classes, reuse=True, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-c3a09e9c02f6>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer (with train logits, for dropout to take effect)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits_train, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.5401, Training Accuracy= 0.670\n",
      "Step 100, Minibatch Loss= 0.1398, Training Accuracy= 0.940\n",
      "Step 200, Minibatch Loss= 0.0462, Training Accuracy= 0.960\n",
      "Step 300, Minibatch Loss= 0.0308, Training Accuracy= 0.990\n",
      "Step 400, Minibatch Loss= 0.0378, Training Accuracy= 0.990\n",
      "Step 500, Minibatch Loss= 0.0303, Training Accuracy= 0.990\n",
      "Step 600, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "Step 700, Minibatch Loss= 0.0667, Training Accuracy= 0.970\n",
      "Step 800, Minibatch Loss= 0.0317, Training Accuracy= 0.990\n",
      "Step 900, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Step 1000, Minibatch Loss= 0.0856, Training Accuracy= 0.980\n",
      "Step 1100, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "Step 1200, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "Step 1300, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Step 1400, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "Step 1500, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Step 1600, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "Step 1700, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "Step 1800, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "Step 1900, Minibatch Loss= 0.0299, Training Accuracy= 0.990\n",
      "Step 2000, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for step in range(1, num_steps + 1):\n",
    "\n",
    "    try:\n",
    "        # Run optimization\n",
    "        sess.run(train_op)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # Reload the iterator when it reaches the end of the dataset\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})\n",
    "        sess.run(train_op)\n",
    "\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        # (note that this consume a new batch of data)\n",
    "        loss, acc = sess.run([loss_op, accuracy])\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
