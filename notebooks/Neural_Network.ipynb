{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "n_input = 28\n",
    "n_classes = 2\n",
    "learning_rate = 0.0001\n",
    "num_steps = 75\n",
    "batch_size = 100\n",
    "display_step = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicedata(data, dataset):\n",
    "    trainset = data[dataset]\n",
    "    features = np.float32(trainset[:,1:-2])\n",
    "    labels_int = np.int32(trainset[:,-1])\n",
    "    labels = np.zeros((labels_int.shape[0], n_classes))\n",
    "    labels[range(labels_int.shape[0]), labels_int] = 1\n",
    "    return features, labels\n",
    "    \n",
    "with np.load(DATA_DIR+\"ccdataset.npz\") as data:\n",
    "    features, labels = slicedata(data, 'train')\n",
    "    features_valid, labels_valid = slicedata(data, 'valid')\n",
    "    features_test, labels_test = slicedata(data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels_placeholder = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "X, Y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelname = \"test\"\n",
    "def Baseline_model(x, n_classes, reuse, is_training):\n",
    "    with tf.variable_scope('Baseline', reuse=reuse):\n",
    "        layer1 = tf.layers.dense(inputs=x, units=150, activation=tf.nn.leaky_relu)\n",
    "        layer2 = tf.layers.dense(inputs=layer1, units=150, activation=tf.nn.leaky_relu)\n",
    "        layer3 = tf.layers.dense(inputs=layer2, units=150, activation=tf.nn.leaky_relu)\n",
    "        layer4 = tf.layers.dense(inputs=layer3, units=150, activation=tf.nn.leaky_relu)\n",
    "        #layer5 = tf.layers.dense(inputs=layer4, units=150, activation=tf.nn.leaky_relu)\n",
    "        out = tf.layers.dense(inputs=layer4, units=n_classes)\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_train = Baseline_model(X, n_classes, reuse=False, is_training=True)\n",
    "logits_test = Baseline_model(X, n_classes, reuse=True, is_training=False)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits_train, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "def modeleval(logits, Y):\n",
    "    predicted = tf.argmax(logits, 1)\n",
    "    actual = tf.argmax(Y, 1)\n",
    "\n",
    "    tp = tf.count_nonzero(predicted * actual)\n",
    "    tn = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
    "    fp = tf.count_nonzero(predicted * (actual - 1))\n",
    "    fn = tf.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fmeasure = (2 * precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    return accuracy, fmeasure\n",
    "\n",
    "accuracy, fmeasure = modeleval(logits_test, Y)\n",
    "\n",
    "_features_valid = tf.placeholder(tf.float32, [None, n_input])\n",
    "_labels_valid = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "logits_val = Baseline_model(_features_valid, n_classes, reuse=True, is_training=True)\n",
    "logits_val_test = Baseline_model(_features_valid, n_classes, reuse=True, is_training=False)\n",
    "loss_val_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits_val, labels=_labels_valid))\n",
    "acc_val_op, f1_val_op = modeleval(logits_val_test, _labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Train Loss= 0.5594, Train Acc= 0.670, Train F1= 0.756, Valid Loss= 0.7310, Valid Acc= 0.336, Valid F1= 0.005\n",
      "Step 25, Train Loss= 0.3363, Train Acc= 0.910, Train F1= 0.916, Valid Loss= 0.5892, Valid Acc= 0.876, Valid F1= 0.027\n",
      "Step 50, Train Loss= 0.1867, Train Acc= 0.980, Train F1= 0.981, Valid Loss= 0.3443, Valid Acc= 0.999, Valid F1= 0.721\n",
      "Step 75, Train Loss= 0.1554, Train Acc= 0.940, Train F1= 0.936, Valid Loss= 0.1929, Valid Acc= 0.999, Valid F1= 0.753\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features, \\\n",
    "                                          labels_placeholder: labels})\n",
    "\n",
    "for step in range(1, num_steps + 1):\n",
    "    try:\n",
    "        sess.run(train_op)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})\n",
    "        sess.run(train_op)\n",
    "\n",
    "    loss, acc, f1 = sess.run([loss_op, accuracy, fmeasure])\n",
    "    \n",
    "    loss_val, acc_val, f1_val = sess.run([loss_val_op, acc_val_op, f1_val_op],\\\n",
    "                                         feed_dict={_features_valid: features_test,\\\n",
    "                                                    _labels_valid: labels_test})\n",
    "\n",
    "    stats.append([loss, acc, f1, loss_val, acc_val, f1_val])\n",
    "\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        print(\"Step \" + str(step) + \", Train Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(acc)+ \", Train F1= \" + \\\n",
    "              \"{:.3f}\".format(f1) + \", Valid Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss_val) + \", Valid Acc= \" + \\\n",
    "              \"{:.3f}\".format(acc_val)+ \", Valid F1= \" + \\\n",
    "              \"{:.3f}\".format(f1_val))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.array(stats)\n",
    "np.save(modelname+\".npy\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow1.4",
   "language": "python",
   "name": "tensorflow1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
